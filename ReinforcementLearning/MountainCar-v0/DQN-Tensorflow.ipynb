{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is intended to solve the MountainCar-v0 problem with the vanilla apporach DQN.\n",
    "\n",
    "It will be a benchmark to future algorithms devoloped by Guilherme Viveiros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym #envrionment to test the algorithms\n",
    "import numpy as np #vector calculations\n",
    "import tensorflow as tf #tensor / ML operations\n",
    "import tensorflow.keras as keras  # ML operations\n",
    "import jdc #jupyter dynamic classes\n",
    "import collections #collect experiences from real environment to sample within DQN\n",
    "import random #random environment's\n",
    "import math #math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore this, just plotting settings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Frist let's define our Neural Network to define the policy \n",
    "\n",
    "> As it is the Mountaint Car environmet let's define a simple MLP\n",
    "\n",
    "### Simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MLP(input_dim,hidden_layers,output_dim):\n",
    "        \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Input(shape=(input_dim,)))\n",
    "        \n",
    "    for layer_dim in hidden_layers:\n",
    "        model.add(keras.layers.Dense(units=layer_dim,activation='relu'))\n",
    "            \n",
    "    model.add(keras.layers.Dense(units=output_dim,activation='softmax'))\n",
    "        \n",
    "    model.compile(optimizer='adam',loss='mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's define the Replay Buffer to use within DQN\n",
    "\n",
    "> Also use name tuples to save experiments\n",
    "\n",
    "1. Each experiment contains: \n",
    "    1. State\n",
    "    2. Action\n",
    "    3. Next state\n",
    "    4. Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Experience = namedtuple('Experience',\n",
    "                   ('last_state', 'action', 'state', 'reward','done')\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    #initialize the parameters and the memory buffer\n",
    "    def __init__(self,max_size):\n",
    "        self.memory = []\n",
    "        self.size = 0\n",
    "        self.max_size = max_size\n",
    "        \n",
    "    #push a experience to the memory buffer\n",
    "    def push(self,experience):\n",
    "        #case when the buffer is full\n",
    "        if(self.size >= self.max_size):\n",
    "            self.memory[self.size % self.max_size] = experience\n",
    "       #if it isn't full\n",
    "        else:\n",
    "            self.memory.append(experience)\n",
    "        \n",
    "        self.size +=1\n",
    "    \n",
    "    #check if the memory can provide a batch sample with a specific size\n",
    "    def can_provide_batch(self,batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        return self.size >= self.batch_size\n",
    "    \n",
    "    #return a batch sample\n",
    "    def batch_sample(self):\n",
    "        return random.sample(self.memory,self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's define the Mountain Car Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want reproductible results, so let's define a define seed\n",
    "#I always return state - reward - done in this order\n",
    "class Environment():\n",
    "    #initialize the environment\n",
    "    def __init__(self,env_info={}):\n",
    "        #seed\n",
    "        self.seed = env_info.get(\"sedd\")\n",
    "        self.rand_generator = np.random.seed(seed = self.seed)\n",
    "        #environment\n",
    "        self.env = gym.make('MountainCar-v0')\n",
    "        #renderization\n",
    "        if(env_info.get(\"render\",False)):\n",
    "            self.env.render()\n",
    "    \n",
    "    #initialize the game\n",
    "    def env_start(self):\n",
    "        state = self.env.reset()\n",
    "        #state,reward,done\n",
    "        return (state,0,False)\n",
    "    \n",
    "    #take a step in the game\n",
    "    def env_step(self,action):\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        return (state,reward,done)\n",
    "\n",
    "    #end the game\n",
    "    def env_end(self):\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tested cell\n",
    "env_info = {\n",
    "    \"sedd\" : 3,\n",
    "    \"render\" : False\n",
    "}\n",
    "\n",
    "env = Environment(env_info)\n",
    "assert env.seed == 3\n",
    "\n",
    "state,reward,done = env.env_start()\n",
    "assert reward == 0 , print(\"Agent start error\")\n",
    "assert done == False , print(\"Agent start error\")\n",
    "\n",
    "state,reward,done = env.env_step(1)\n",
    "assert reward == -1 , print(\"Agent step error\")\n",
    "assert done == False , print(\"Agent step error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's define the core part, the Agent , the briliant agent that will solve this game :p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Agent():\n",
    "    #initialize the parameters of the environment\n",
    "    def agent_init(self,agent_info={}):\n",
    "        raise NotImplementedError\n",
    "    #choose and action\n",
    "    def agent_step(self,state):\n",
    "        raise NotImplementedError\n",
    "    #terminal state\n",
    "    def agent_end(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DQN_Agent\n",
    "\n",
    "def agent_init(self,agent_info={}):\n",
    "    #discount factor\n",
    "    self.discount = agent_info.get(\"discount_factor\")\n",
    "    #step-size parameter\n",
    "    self.step_size = agent_info.get(\"step_size\")\n",
    "    #policy_network\n",
    "    self.policy_network = agent_info.get(\"policy_network\")\n",
    "    #target_network\n",
    "    self.target_network = agent_info.get(\"target_network\")\n",
    "    #number of available actions\n",
    "    self.number_actions = agent_info.get(\"number_actions\")\n",
    "    #epsilon-greedy\n",
    "    self.epsilon_greedy = agent_info.get(\"epsilon_greedy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DQN_Agent\n",
    "\n",
    "def agent_step(self,state):\n",
    "    \n",
    "    #exploitation case\n",
    "    if(np.random.random() > self.epsilon_greedy):\n",
    "        action = np.argmax(self.policy_network(state[np.newaxis]).numpy()[0])\n",
    "    #exploration\n",
    "    else:\n",
    "        action = np.random.choice(range(self.number_actions))\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Hyper-Parameters for the DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = 2\n",
    "out_dim = 3\n",
    "hidden_layers = [60,30,20]\n",
    "\n",
    "policy_network = MLP(in_dim,hidden_layers,out_dim)\n",
    "target_network = MLP(in_dim,hidden_layers,out_dim)\n",
    "\n",
    "#set the target networks trainable parameter to false.\n",
    "#Every episode update this weights\n",
    "target_network.trainable = False\n",
    "target_network.set_weights(policy_network.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factor = 0.999\n",
    "step_size = 0.001\n",
    "number_actions = 3\n",
    "epsilon_greedy = 0.1\n",
    "\n",
    "num_iterations = 1000\n",
    "num_episodes = 10\n",
    "\n",
    "max_buffer_size = 1000\n",
    "batch_size = 32\n",
    "\n",
    "#lr = 0.001\n",
    "#optimizer = keras.optimizers.Adam(lr = lr)\n",
    "#target_update = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = {\n",
    "    \"sedd\" : 3,\n",
    "    \"render\" : False\n",
    "}\n",
    "\n",
    "env = Environment(env_info)\n",
    "state,reward,done = env.env_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_info = {\n",
    "    \"discount\" : discount_factor,\n",
    "    \"step_size\" : step_size,\n",
    "    \"policy_network\": policy_network,\n",
    "    \"target_network\": target_network,\n",
    "    \"epsilon_greedy\": epsilon_greedy,\n",
    "    \"number_actions\": number_actions\n",
    "}\n",
    "\n",
    "agent = DQN_Agent()\n",
    "agent.agent_init(agent_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions\n",
    "\n",
    "1. Get the q value for the current state\n",
    "2. Get the q value for the next state, predicted by the target network\n",
    "3. Extract the tensors from the batch, respectivelly:\n",
    "    > State\\\n",
    "    > Next state\\\n",
    "    > Reward\\\n",
    "    > Action\\\n",
    "    > Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss functions\n",
    "from keras.losses import MSE\n",
    "\n",
    "'''\n",
    "#Steps \n",
    "1. Iterate over the \"current\" states from the batch. When I say current it's the actual taken states by the agent\n",
    "2. For each state, check the action that the agent performed\n",
    "3. Extract the q value from the ploicy network for that state following the action\n",
    "'''\n",
    "def get_current_q(states,actions):\n",
    "    \n",
    "    q_value = []\n",
    "    count = 0\n",
    "    \n",
    "    for state in states:\n",
    "        \n",
    "        #action selected in s\n",
    "        action_selected = actions[count]\n",
    "        \n",
    "        #extract the float number from the tensorflow tensor\n",
    "        #and the q value with respect to the chosen action\n",
    "        q = policy_network(state[np.newaxis])[0][action_selected]\n",
    "        q_value.append(q)\n",
    "        \n",
    "        count+=1\n",
    "        \n",
    "    return q_value\n",
    "\n",
    "'''\n",
    "#Steps \n",
    "1. Same logic as with get_current_q, with the exception that if it's the terminal state the q value is 0.\n",
    "2. I'm using a greedy \n",
    "'''\n",
    "def get_q_prime(next_states,done):\n",
    "    \n",
    "    q_value = []\n",
    "    count=0\n",
    "    \n",
    "    for state in next_states:\n",
    "        \n",
    "        #need to check if it's the last state, for this I use the bool returned by the Envrionment \n",
    "        if[done[count] == True]:\n",
    "            q_value.append(tf.Variable(0))\n",
    "        else:\n",
    "            q = target_network(s[np.newaxis])[0] #extract the float number from the tensorflow tensor\n",
    "            q_value.append(tf.reduce_max(q))\n",
    "        \n",
    "        count+=1\n",
    "    \n",
    "    return q_value\n",
    "\n",
    "def extract_tensor(experiences):\n",
    "    \n",
    "    experiences = Experience(*zip(*experiences))\n",
    "    \n",
    "    last_state = experiences.last_state\n",
    "    reward = experiences.reward\n",
    "    state = experiences.state\n",
    "    action = experiences.action\n",
    "    done = experiences.done\n",
    "    \n",
    "    return last_state,action,state,reward,done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotRewards():\n",
    "    def __init__(self,plot_info):\n",
    "    \n",
    "        self.epi = plot_info\n",
    "        self.sum_reward_per_episode = []\n",
    "\n",
    "    def plot(self,reward_per_episode):\n",
    "        plt.figure(2)\n",
    "        plt.clf()\n",
    "        plt.title('Training --- Each iteration has 10 episodes inside')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Duration')\n",
    "\n",
    "       \n",
    "        self.sum_reward_per_episode.append(reward_per_episode*-1)\n",
    "\n",
    "        #Plot the sum reward of every episode in each iteration that consis$\n",
    "        plt.plot(self.sum_reward_per_episode,label='Mean Reward per Episode');\n",
    "        plt.legend();\n",
    "        \n",
    "        #if the user want to check an average window of 10, put it innto the di$\n",
    "        if(len(self.sum_reward_per_episode) % self.epi == 0):\n",
    "            l = self.sum_reward_per_episode[-self.epi:]\n",
    "            plt.plot([l for i in range(self.epi)],c='r',label='Mean Reward per iteration');\n",
    "            plt.legend();\n",
    "            \n",
    "        plt.pause(0.001)\n",
    "        print(\"Episode \", len(self.sum_reward_per_episode))\n",
    "        if is_ipython: display.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will create a folder that will retain the weights of the policy network every 10 iterations\n",
    "import os\n",
    "if not os.path.exists('dqn_agent_weights'):\n",
    "    os.mkdir('dqn_agent_weights')\n",
    "\n",
    "file = 'dqn_agent_weights/weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Environment' object has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-6920d90bc425>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#model.save(file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-bd570cf365ea>\u001b[0m in \u001b[0;36menv_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#end the game\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0menv_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Environment' object has no attribute 'close'"
     ]
    }
   ],
   "source": [
    "#have the agent and the environment all set up\n",
    "\n",
    "#plot class with a window average of 10\n",
    "plot = PlotRewards(10)\n",
    "\n",
    "#Initialize the repllayBuffer\n",
    "replay_buffer = ReplayBuffer(max_buffer_size)\n",
    "\n",
    "#for each iteration -> 10 experience and average the results (less variance)\n",
    "for iteration in range(num_iterations):\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        #all the rewards received in one episode\n",
    "        rewards_per_episode = []\n",
    "        \n",
    "        #start state\n",
    "        state,reward,done = env.env_start()\n",
    "        \n",
    "        #Iterate until the terminal state\n",
    "        while done != True:\n",
    "            \n",
    "            #Agent choose the action according to the policy network and the current state\n",
    "            action = agent.agent_step(state)\n",
    "            \n",
    "            #new state after executing the action chosen by the agent\n",
    "            next_state,reward,done = env.env_step(action)\n",
    "\n",
    "            #append the rewards \n",
    "            rewards_per_episode.append(reward)\n",
    "           \n",
    "        \n",
    "            #make an experience\n",
    "            experience = Experience(state,action,next_state,reward,done)\n",
    "            #add the experience to the replay buffer\n",
    "            replay_buffer.push(experience)\n",
    "        \n",
    "            #When collected 32(batch size) experiences\n",
    "            if(replay_buffer.can_provide_batch(batch_size)):\n",
    "                \n",
    "                #batch a sample of data\n",
    "                experiences_buffer = replay_buffer.batch_sample()\n",
    "                \n",
    "                #extract all the features of this experience batch\n",
    "                states,actions,next_states,rewards,dones = extract_tensor(experiences_buffer)\n",
    "            \n",
    "                #From now one we need a gradient tape to calculate the gradients of the policy with respect to the loss\n",
    "                #The loss is the MSE between the Q function and the Q' function\n",
    "                with tf.GradientTape() as tape:\n",
    "                    #Get the q values for the taken states and actions\n",
    "                    current_q = get_current_q(states,actions)\n",
    "            \n",
    "                    #Get the q prime values for the next states computed by the target network\n",
    "                    q_prime = get_q_prime(next_states,dones)\n",
    "    \n",
    "                    #compute the target error\n",
    "                    target_error = tf.convert_to_tensor(rewards) + tf.constant(discount_factor,dtype= tf.float32) * q_prime\n",
    "            \n",
    "                    #compute the loss function. I will use the Mean Squared error as the loss\n",
    "                    loss = MSE(target_error,current_q)\n",
    "            \n",
    "                    #compute the gradients with respect to the policy_network\n",
    "                    grads = tape.gradient(loss,policy_network.trainable_variables)\n",
    "                \n",
    "                    #Then aplly the gradients\n",
    "                    policy_network.optimizer.apply_gradients(zip(grads, policy_network.trainable_variables))\n",
    "  \n",
    "        #1 in 1 episode -> plot the rewards as a function of time\n",
    "        plot.plot(np.sum(np.asarray(rewards_per_episode)))\n",
    "        \n",
    "    #ended the X episodes, e.g, one iteration terminated\n",
    "    #each iteration I  update the target weights\n",
    "    target_network.set_weights(policy_network.get_weights())\n",
    "        \n",
    "    if(iteration % 10 == 0):\n",
    "        tmp = file+str(iteration)+'.hdf5'\n",
    "        model.save(tmp)\n",
    "\n",
    "env.env_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
