{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is intended to solve the CartPole-v0 problem with the Advantage Actor Critic (A2C).\n",
    "\n",
    "It will be a benchmark to future algorithms devoloped by Guilherme Viveiros.\n",
    "\n",
    "\n",
    "> Solved Requirements for CartPole: Considered solved when the average return is greater than or equal to 195.0 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym #envrionment to test the algorithms\n",
    "import numpy as np #vector calculations\n",
    "import tensorflow as tf #tensor / ML operations\n",
    "#pip install jdc\n",
    "import os #operative system\n",
    "from tqdm import tqdm #progress bar\n",
    "from tqdm import trange\n",
    "\n",
    "import collections #collect experiences from real environment to sample within DQN\n",
    "import random #random environment's\n",
    "import math#math\n",
    "import time #time \n",
    "\n",
    "from typing import Any, List, Sequence, Tuple #to define custom function returns\n",
    "\n",
    "# Small epsilon value for stabilizing division operations\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole-V0 environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's define the core part, the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.models.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        number_of_actions : int,\n",
    "        number_hidden_units : int\n",
    "        #**kargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(number_hidden_units,activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(number_hidden_units*2,activation='relu')\n",
    "        self.actor =  tf.keras.layers.Dense(number_of_actions,name='Actor')\n",
    "        self.critic =  tf.keras.layers.Dense(1,name='Critic')\n",
    "    \n",
    "    #return the action logits as the actor and the value state as the critic\n",
    "    def call(self,inputs : tf.Tensor) -> Tuple[tf.Tensor,tf.Tensor]:\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return self.actor(x),self.critic(x)\n",
    "    \n",
    "action_space = env.action_space.n\n",
    "number_hidden_units = 64\n",
    "model = ActorCritic(action_space,number_hidden_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform an action and change to another state\n",
    "\n",
    "> Also, tf_env_step it's the wrapper function that transforms env_step into a tensorflow function to be added in tf.graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the next state, the associated reward and a boolean indicating if the agent reached the terminal state\n",
    "#I receive and output arrays because I'm using tf.numpy_function to wrap env_step to graph_mode and this functions\n",
    "#expects that the python function receives as its arguments an array and returns arrays as its outputs\n",
    "def env_step(action : np.array) -> Tuple[np.array,np.array,np.array]:\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    return ( state.astype(np.float32),\n",
    "             np.array(reward,dtype=np.int32),\n",
    "             np.array(done,dtype=np.int32)\n",
    "    )\n",
    "    \n",
    "def tf_env_step(action : tf.Tensor ) -> List[tf.Tensor]:\n",
    "    return tf.numpy_function(func = env_step, inp = [action], Tout = [tf.float32,tf.int32,tf.int32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return array containing rewards, values and the associated action probabilities of each state\n",
    "#.mark_used() in graph time as to be gone\n",
    "def run_episode(initial_state : tf.Tensor , model : tf.keras.Model, max_steps : int ) -> List[tf.Tensor]:\n",
    "    \n",
    "    rewards = tf.TensorArray(dtype = tf.int32, size = 0, dynamic_size=True)\n",
    "    values = tf.TensorArray(dtype = tf.float32, size = 0, dynamic_size=True)\n",
    "    action_probs_t = tf.TensorArray(dtype = tf.float32, size = 0, dynamic_size=True)\n",
    "    \n",
    "    \n",
    "    initial_state_shape = initial_state.shape\n",
    "    state = initial_state\n",
    "    \n",
    "    #Iterate until the terminal state, True == 1\n",
    "    for step in tf.range(max_steps):\n",
    "        \n",
    "        state = tf.expand_dims(state,0)\n",
    "        \n",
    "        #Agent choose the action according to the actor network\n",
    "        action_logits, state_value = model(state)\n",
    "        \n",
    "        #use multionomial function from tensorflow probability to sample a given action from action_logits\n",
    "        action = tf.random.categorical(logits=action_logits,num_samples=1)[0,0]\n",
    "        \n",
    "        #action probabilities\n",
    "        action_probs = tf.nn.softmax(action_logits)\n",
    "        \n",
    "        #append value and proability actions associated\n",
    "        values = values.write(step,tf.squeeze(state_value))#.mark_used\n",
    "        action_probs_t = action_probs_t.write(step,action_probs[0,action])#.mark_used\n",
    "        \n",
    "        #new state after executing the action chosen by the agent\n",
    "        state,reward,done = tf_env_step(action)\n",
    "        #state = tf.reshape(state,[1,4])\n",
    "        state.set_shape(initial_state_shape)\n",
    "        \n",
    "                \n",
    "        #append reward associated\n",
    "        rewards = rewards.write(step,reward)#.mark_used\n",
    "\n",
    "        \n",
    "        if tf.cast(done,tf.bool):\n",
    "            break\n",
    "        \n",
    "    action_probs_t = action_probs_t.stack()\n",
    "    values = values.stack()\n",
    "    rewards = rewards.stack()\n",
    "        \n",
    "    return rewards,values,action_probs_t\n",
    "\n",
    "#rewards,values,action_probs = run_episode(tf.constant(env.reset(), dtype = tf.float32) , model, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage Actor Critic with Monte Carlo Updates\n",
    "\n",
    "In this case instead of $Q(at,st)$ and  subsequently $ Rt + V(st') $ is replaced by $Gt$, the actual final reward.\n",
    "   \n",
    "> $A(at,st) = Q(at,st) - V(st)$\n",
    "  \n",
    " Since it useless to define a DNN to predict value states and q values (value-action pairs), we can use the Bellman equations optimallity in our advantage\n",
    "  \n",
    "  > $ Q*(at,st) = E[Rt + V*(st')] $ (1)\n",
    "  \n",
    " So\n",
    "  \n",
    ">$ A(at,st) = Rt + V(st') - V(st) $ (2) when using TD(0) updates\\\n",
    ">$ A(at,st) = Gt - V(st) $ (2) when using MC updates\n",
    "\n",
    "And this is simply the TD Error\n",
    "  \n",
    "  Now I only need a NN to predict value states.\n",
    "  \n",
    "  The $E$ in equation (1) stands for expectation, it's the expectation cumulative reward of follwing $at$ in $st$. It's removed in equation **2** because I'm using TD(0), a boostrap method. Since I sample every step following the current policy I'm receiving the true rewards, thats why we can remove the expectation symbol in (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.mark_used() in graph time as to be gone\n",
    "def compute_cumulative_rewards(\n",
    "        rewards : tf.Tensor,\n",
    "        gamma : float,\n",
    "        standardize : bool = True\n",
    "    ) -> [tf.Tensor]:\n",
    "    \n",
    "    #G(t) = R(t+1) + γ*R(t+2) + γ^2*R(t+3) + ...\n",
    "    #Cumulative reward -> G(t) = R + y*G(t+1)\n",
    "    \n",
    "    n = tf.shape(rewards)[0]\n",
    "    rewards = tf.cast(rewards[::-1],tf.float32)\n",
    "    \n",
    "    \n",
    "    cumulative_returns = tf.TensorArray(dtype=tf.float32,size = n)\n",
    "    gamma = tf.cast(gamma,tf.float32)\n",
    "    \n",
    "    cumulative_reward = tf.constant(0.0)\n",
    "    cumulative_reward_shape = cumulative_reward.shape\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in tf.range(n):\n",
    "        cumulative_reward = rewards[i] + gamma * cumulative_reward\n",
    "        #to ensure a known shape in graph time\n",
    "        cumulative_reward.set_shape(cumulative_reward_shape)\n",
    "        cumulative_returns = cumulative_returns.write(i,cumulative_reward)#.mark_used() \n",
    "        \n",
    "    \n",
    "    cumulative_returns = cumulative_returns.stack()[::-1]\n",
    "    \n",
    "    if(standardize):\n",
    "        mean,std = tf.math.reduce_mean(cumulative_returns),tf.math.reduce_std(cumulative_returns)\n",
    "        #use epsilon to ensure non-zero divisions\n",
    "        cumulative_returns = (cumulative_returns - mean)/(std + eps)\n",
    "    \n",
    "    return cumulative_returns\n",
    "\n",
    "#cumulative_returns = compute_cumulative_rewards(rewards,gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will use the huber loss as the critic loss\n",
    "#as reductin technique, instead of aggregating and computing the mean, I sum up the errors of each step within an episode\n",
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def compute_loss(\n",
    "        rewards : tf.Tensor,\n",
    "        values : tf.Tensor,\n",
    "        action_probs : tf.Tensor\n",
    "    ) ->  tf.Tensor :\n",
    "    \"\"\" Computes the combined actor-critic loss with advantage as the baseline\"\"\"\n",
    "    \n",
    "    #compute the associated advantage \n",
    "    advantage = rewards - values\n",
    "    \n",
    "    #actor loss\n",
    "    log_action = tf.math.log(action_probs)\n",
    "    actor_loss = -tf.reduce_sum(log_action * advantage)\n",
    "    \n",
    "    #critic loss\n",
    "    critic_loss = huber_loss(rewards,values)\n",
    "    \n",
    "    return actor_loss + critic_loss\n",
    "\n",
    "loss = compute_loss(cumulative_returns,values,action_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Part\n",
    "\n",
    "> Defining the training step to update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "@tf.function\n",
    "def train_step(\n",
    "        model : tf.keras.Model,\n",
    "        initial_state : tf.Tensor,\n",
    "        optimizer : tf.keras.optimizers.Optimizer,\n",
    "        gamma : float,\n",
    "        max_steps : int\n",
    "    ) -> tf.Tensor:\n",
    "    \"\"\" Runs a model training step \"\"\"\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        #tape.watch(model.trainable_variables)\n",
    "        #run an episode\n",
    "        #action_probs, values, rewards = run_episode(initial_state,model,max_steps)\n",
    "        rewards, values, action_probs = run_episode(initial_state,model,max_steps)\n",
    "        \n",
    "        #cumpute the cumulative rewards\n",
    "        #cumulative_rewards = compute_cumulative_rewards(rewards,gamma)\n",
    "        cumulative_rewards = compute_cumulative_rewards(rewards,gamma)\n",
    "        \n",
    "        #Convert training data to appropriate TF tensor shapes\n",
    "        #the previous return as the shape -> (steps,) , so ensure to change it to (steps,1)\n",
    "        action_probs, values, cumulative_rewards = [\n",
    "            tf.expand_dims(x, 1) for x in [action_probs, values, cumulative_rewards]] \n",
    "        \n",
    "        #compute the loss\n",
    "        loss = compute_loss(cumulative_rewards,values,action_probs)        \n",
    "    \n",
    "    #comput the gradients \n",
    "    gradients = tape.gradient(loss,model.trainable_variables)\n",
    "    #apply the gradients\n",
    "    optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "    \n",
    "    #return the associated reward\n",
    "    episode_reward = tf.math.reduce_sum(rewards)\n",
    "    \n",
    "    return episode_reward\n",
    "\n",
    "#reward = train_step(model,tf.constant(env.reset(),dtype=tf.float32),optimizer,tf.Variable(0.99),20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_directory = \"/tmp/training_checkpoints\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    optimizer=optimizer,\n",
    "    actor_critic_model = model,\n",
    ")\n",
    "\n",
    "# saving (checkpoint) the model every 2 epochs\n",
    "#checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "#restore the last checkpoint\n",
    "#status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 3474:  35%|███▍      | 3474/10000 [01:02<01:57, 55.58it/s, episode_reward=200, weighted_moving_average=195]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solved at episode 3474: average reward: 195.03!\n",
      "CPU times: user 1min 14s, sys: 4.33 s, total: 1min 18s\n",
      "Wall time: 1min 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "max_episodes = 10000\n",
    "max_steps_per_episode = 200\n",
    "gamma = 0.99\n",
    "weighted_moving_average = 0\n",
    "\n",
    "reward_threshold = 195\n",
    "\n",
    "with trange(max_episodes) as t:\n",
    "    #each episode\n",
    "    for i in t:\n",
    "        #retrieve the initial state\n",
    "        initial_state = tf.constant(env.reset(),dtype=tf.float32)\n",
    "        #episode reward\n",
    "        episode_reward = int (train_step(model,initial_state,optimizer,gamma,max_steps_per_episode))\n",
    "        \n",
    "        weighted_moving_average = episode_reward * 0.01 + weighted_moving_average * 0.99\n",
    "        \n",
    "        '''\n",
    "        #exponential scheduling\n",
    "        if(i % 200 == 0 and i > 0 and optimizer.learning_rate >= 1e-4):\n",
    "            optimizer.learning_rate  = optimizer.learning_rate * 0.1\n",
    "            tf.print(\"Learning changed to {0}\".format(optimizer.learning_rate))\n",
    "        '''\n",
    "        \n",
    "        t.set_description(f'Episode {i}')\n",
    "        t.set_postfix(\n",
    "            episode_reward=episode_reward, weighted_moving_average=weighted_moving_average\n",
    "        )\n",
    "        \n",
    "        #I'm using a weighted moving average with B = 0.99, so it is equivelent to an average over 100 episodes\n",
    "        if(weighted_moving_average >= reward_threshold):  \n",
    "            break\n",
    "\n",
    "print(f'\\nSolved at episode {i}: average reward: {weighted_moving_average:.2f}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "## solved at episode 659, best so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Visiualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n",
      "Episode finished after 200 timesteps\n"
     ]
    }
   ],
   "source": [
    "num_episodes_to_display = 10\n",
    "for i_episode in range(num_episodes_to_display):\n",
    "    state = env.reset()\n",
    "    for t in range(200):\n",
    "        env.render()\n",
    "        state = tf.expand_dims(tf.constant(state,dtype=tf.float32),0)\n",
    "        action_logits, state_value = model(state)  \n",
    "        #use multionomial function from tensorflow probability to sample a given action from action_logits\n",
    "        action = tf.random.categorical(logits=action_logits,num_samples=1)[0,0]\n",
    "        \n",
    "        state, reward, done, info = env.step(action.numpy())\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render an episode and save as a GIF file\n",
    "\n",
    "> Not working yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "EasyProcessError",
     "evalue": "start error <EasyProcess cmd_param=['Xvfb', '-help'] cmd=['Xvfb', '-help'] oserror=[Errno 2] No such file or directory: 'Xvfb' return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/easyprocess/__init__.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             self.popen = subprocess.Popen(\n\u001b[0m\u001b[1;32m    169\u001b[0m                 \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    855\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1701\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Xvfb'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEasyProcessError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-0e9bae4c4242>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyvirtualdisplay/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, backend, visible, size, color_depth, bgcolor, use_xauth, retries, extra_args, manage_global_env, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown backend: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         self._obj = cls(\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mcolor_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor_depth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyvirtualdisplay/xvfb.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size, color_depth, bgcolor, use_xauth, fbdir, dpi, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         AbstractDisplay.__init__(\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mPROGRAM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyvirtualdisplay/abstractdisplay.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, program, use_xauth, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retries_current\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mhelptext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_helptext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_displayfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"-displayfd\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhelptext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_displayfd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyvirtualdisplay/util.py\u001b[0m in \u001b[0;36mget_helptext\u001b[0;34m(program)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_stdout_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_stderr_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mhelptext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhelptext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/easyprocess/__init__.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \"\"\"\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/easyprocess/__init__.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OSError exception: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moserror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moserror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moserror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEasyProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"start error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"process was started (pid=%s)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEasyProcessError\u001b[0m: start error <EasyProcess cmd_param=['Xvfb', '-help'] cmd=['Xvfb', '-help'] oserror=[Errno 2] No such file or directory: 'Xvfb' return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
     ]
    }
   ],
   "source": [
    "# Render an episode and save as a GIF file\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "from PIL import Image\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()\n",
    "\n",
    "\n",
    "def render_episode(env: gym.Env, model: tf.keras.Model, max_steps: int): \n",
    "    \n",
    "    screen = env.render(mode='rgb_array')\n",
    "    im = Image.fromarray(screen)\n",
    "    \n",
    "    images = [im]\n",
    "\n",
    "    state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "    \n",
    "    for i in range(1, max_steps + 1):\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        action_probs, _ = model(state)\n",
    "        action = np.argmax(np.squeeze(action_probs))\n",
    "\n",
    "        state, _, done, _ = env.step(action)\n",
    "        state = tf.constant(state, dtype=tf.float32)\n",
    "\n",
    "        # Render screen every 10 steps\n",
    "        if i % 10 == 0:\n",
    "            screen = env.render(mode='rgb_array')\n",
    "            images.append(Image.fromarray(screen))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "# Save GIF image\n",
    "images = render_episode(env, model, 200)\n",
    "#image_file = 'cartpole-v0.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "#images[0].save(\n",
    "#    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)\n",
    "\n",
    "#import tensorflow_docs.vis.embed as embed\n",
    "#embed.embed_file(image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
