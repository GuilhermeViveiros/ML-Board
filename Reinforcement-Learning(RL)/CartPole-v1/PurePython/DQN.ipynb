{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is intended to solve the CartPole-v0 problem with the vanilla apporach DQN.\n",
    "\n",
    "It will be a benchmark to future algorithms devoloped by Guilherme Viveiros.\n",
    "\n",
    "\n",
    "> Solved Requirements for CartPole: Considered solved when the average return is greater than or equal to 195.0 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym #envrionment to test the algorithms\n",
    "import numpy as np #vector calculations\n",
    "import tensorflow as tf #tensor / ML operations\n",
    "#!pip install jdc\n",
    "import jdc #jupyter dynamic classes\n",
    "import collections #collect experiences from real environment to sample within DQN\n",
    "import random #random environment's\n",
    "import math#math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore this, just plotting settings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Frist let's define our Neural Network to define the policy \n",
    "\n",
    "> As it is the Mountaint Car environmet let's define a simple MLP\n",
    "\n",
    "### Simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import L2\n",
    "\n",
    "#MLP to CartoPole-v0 only outputs one action-state value, since it\n",
    "def MLP(input_dim,hidden_layers,output_dim):\n",
    "        \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(input_dim,)))\n",
    "        \n",
    "    for layer_dim in hidden_layers:\n",
    "        model.add(tf.keras.layers.Dense(units=layer_dim,activation='relu'))\n",
    "            \n",
    "    model.add(tf.keras.layers.Dense(units=output_dim,activation='linear'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's define the Replay Buffer to use within DQN\n",
    "\n",
    "> Also use name tuples to save experiments\n",
    "\n",
    "1. Each experiment contains: \n",
    "    1. State\n",
    "    2. Action\n",
    "    3. Next state\n",
    "    4. Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Experience = namedtuple('Experience',\n",
    "                   ('last_state', 'action', 'state', 'reward','done')\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    #initialize the parameters and the memory buffer\n",
    "    def __init__(self,max_size):\n",
    "        self.memory = []\n",
    "        self.size = 0\n",
    "        self.max_size = max_size\n",
    "        \n",
    "    #push a experience to the memory buffer\n",
    "    def push(self,experience):\n",
    "        #case when the buffer is full\n",
    "        if(self.size >= self.max_size):\n",
    "            self.memory[self.size % self.max_size] = experience\n",
    "       #if it isn't full\n",
    "        else:\n",
    "            self.memory.append(experience)\n",
    "        \n",
    "        self.size +=1\n",
    "    \n",
    "    #check if the memory can provide a batch sample with a specific size\n",
    "    def can_provide_batch(self,batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        return self.size >= self.batch_size\n",
    "    \n",
    "    #return a batch sample\n",
    "    def batch_sample(self):\n",
    "        return random.sample(self.memory,self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's define the Mountain Car Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want reproductible results, so let's define a define seed\n",
    "#I always return state - reward - done in this order\n",
    "class Environment():\n",
    "    #initialize the environment\n",
    "    def __init__(self,env_info={}):\n",
    "        #seed\n",
    "        #self.seed = env_info.get(\"sedd\")\n",
    "        #self.rand_generator = np.random.seed(seed = self.seed)\n",
    "        #environment\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        #renderization\n",
    "        if(env_info.get(\"render\",False)):\n",
    "            self.env.render()\n",
    "    \n",
    "    #initialize the game\n",
    "    def env_start(self):\n",
    "        state = self.env.reset()\n",
    "        #state,reward,done\n",
    "        return (state,0,False)\n",
    "    \n",
    "    #take a step in the game\n",
    "    def env_step(self,action):\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        return (state,reward,done)\n",
    "\n",
    "    #end the game\n",
    "    def env_end(self):\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tested cell\n",
    "env_info = {\n",
    "    \"render\" : False\n",
    "}\n",
    "\n",
    "env = Environment(env_info)\n",
    "#assert env.seed == 3\n",
    "\n",
    "state,reward,done = env.env_start()\n",
    "assert reward == 0 , print(\"Agent start error\")\n",
    "assert done == False , print(\"Agent start error\")\n",
    "\n",
    "state,reward,done = env.env_step(1)\n",
    "assert reward == 1 , print(\"Agent step error\")\n",
    "assert done == False , print(\"Agent step error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's define the core part, the Agent , the briliant agent that will solve this game :p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class DQN_Agent():\n",
    "    #initialize the parameters of the environment\n",
    "    def agent_init(self,agent_info={}):\n",
    "        raise NotImplementedError\n",
    "    #choose and action\n",
    "    def agent_step(self,state):\n",
    "        raise NotImplementedError\n",
    "    #terminal state\n",
    "    def agent_end(self):\n",
    "        raise NotImplementedError\n",
    "    #rate to explore\n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * math.exp(-1. * current_step * self.decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> All the parameters needed by the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DQN_Agent\n",
    "\n",
    "def agent_init(self,agent_info={}):\n",
    "    #discount factor\n",
    "    self.discount = agent_info.get(\"discount\")\n",
    "    \n",
    "    #step-size parameter\n",
    "    self.step_size = agent_info.get(\"step_size\")\n",
    "    #number of available actions\n",
    "    self.number_actions = agent_info.get(\"number_actions\")\n",
    "    \n",
    "    #epsilon-greedy - Start , End and a Decay\n",
    "    self.start = agent_info.get(\"epsilon_start\")\n",
    "    self.end = agent_info.get(\"epsilon_end\")\n",
    "    self.decay = agent_info.get(\"epsilon_decay\")\n",
    "    #current-step\n",
    "    self.current_step = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to DQN_Agent\n",
    "\n",
    "def agent_step(self,state):\n",
    "    \n",
    "    rate = self.get_exploration_rate(self.current_step)\n",
    "    self.current_step += 1\n",
    "    \n",
    "    #exploitation case\n",
    "    if(random.random() > rate):\n",
    "        action = np.argmax(tf.stop_gradient(policy_network(state[np.newaxis]),name=\"Choosen-State\").numpy()[0])\n",
    "    #exploration\n",
    "    else:\n",
    "        action = random.randrange(self.number_actions) # explore \n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Hyper-Parameters for the DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 2,466\n",
      "Trainable params: 2,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "in_dim = 4\n",
    "out_dim = 2\n",
    "hidden_layers = [64,32]\n",
    "\n",
    "policy_network = MLP(in_dim,hidden_layers,out_dim)\n",
    "#policy_network.build(tf.TensorShape([None, 4]))\n",
    "\n",
    "target_network = MLP(in_dim,hidden_layers,out_dim)\n",
    "#target_network.build(tf.TensorShape([None, 4]))\n",
    "\n",
    "#set the target networks trainable parameter to false.\n",
    "#Every episode update this weights\n",
    "target_network.trainable = False\n",
    "target_network.set_weights(policy_network.get_weights())\n",
    "\n",
    "policy_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.0005\n",
    "\n",
    "discount_factor = 0.99\n",
    "number_actions = out_dim\n",
    "\n",
    "num_iterations = 1000\n",
    "num_episodes = 10\n",
    "\n",
    "max_buffer_size = 100000\n",
    "batch_size = 64 #(was 256)\n",
    "\n",
    "step_size = 0.0007\n",
    "optimizer = tf.keras.optimizers.Adam(lr = step_size)\n",
    "#target_update = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = {\n",
    "    \"sedd\" : 4,\n",
    "    \"render\" : False\n",
    "}\n",
    "\n",
    "env = Environment(env_info)\n",
    "#state,reward,done = env.env_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_info = {\n",
    "    \"discount\" : discount_factor,\n",
    "    \"step_size\" : step_size,\n",
    "    \"epsilon_start\": eps_start,\n",
    "    \"epsilon_end\": eps_end,\n",
    "    \"epsilon_decay\": eps_decay,\n",
    "    \"number_actions\": number_actions\n",
    "}\n",
    "\n",
    "agent = DQN_Agent()\n",
    "agent.agent_init(agent_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotRewards():\n",
    "    def __init__(self,plot_info,agent):\n",
    "    \n",
    "        self.epi = plot_info\n",
    "        self.sum_reward_per_episode = []\n",
    "        self.mean_reward = []\n",
    "\n",
    "    def plot(self,reward_per_episode):\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.clf()\n",
    "        plt.title('Training --- Each iteration has 10 episodes inside')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Duration')\n",
    "\n",
    "       \n",
    "        self.sum_reward_per_episode.append(reward_per_episode)\n",
    "        \n",
    "        #check if the game requirements are completed - 195.0 over 100\n",
    "        if(len(self.sum_reward_per_episode) >= 100):\n",
    "            tmp = self.self.sum_reward_per_episode[-100:] >= 195\n",
    "            if(tmp.sum() == 100):\n",
    "                print(\"Completeded\")\n",
    "                return True\n",
    "        \n",
    "        #if the user want to check an average window of 10, put it innto the dict\n",
    "        if(len(self.sum_reward_per_episode) >= self.epi):\n",
    "            \n",
    "            #moving average of epi\n",
    "            self.mean_reward.append(\n",
    "                np.mean(\n",
    "                    self.sum_reward_per_episode[-self.epi:]\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            plt.plot(self.mean_reward,label='Moving Average',c='r');\n",
    "            \n",
    "        \n",
    "        #Plot the sum reward of every episode in each iteration that consis$\n",
    "        plt.plot(self.sum_reward_per_episode,label='Mean Reward per Episode');\n",
    "        plt.legend(loc='upper right',fontsize='large');\n",
    "            \n",
    "        plt.pause(0.001)\n",
    "        rate = agent.get_exploration_rate(agent.current_step)\n",
    "        print(\"Episode \" + str(len(self.sum_reward_per_episode)) + \" With epsilon of : \" + str(rate))\n",
    "        if is_ipython: display.clear_output(wait=True)\n",
    "        \n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions\n",
    "\n",
    "1. Get the q value for the current state\n",
    "2. Get the q value for the next state, predicted by the target network\n",
    "3. Extract the tensors from the batch, respectivelly:\n",
    "    > State\\\n",
    "    > Next state\\\n",
    "    > Reward\\\n",
    "    > Action\\\n",
    "    > Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss functions\n",
    "from keras.losses import MSE\n",
    "\n",
    "'''\n",
    "#Steps \n",
    "1. Iterate over the \"current\" states from the batch. When I say current it's the actual taken states by the agent\n",
    "2. For each state, check the action that the agent performed\n",
    "3. Extract the q value from the ploicy network for that state following the action\n",
    "'''\n",
    "def get_current_q(states,actions):\n",
    "    \n",
    "    q_value = []\n",
    "    count = 0\n",
    "    \n",
    "    for state in states:\n",
    "        \n",
    "        #action selected in s\n",
    "        action_selected = actions[count]\n",
    "        \n",
    "        #extract the float number from the tensorflow tensor\n",
    "        #and the q value with respect to the chosen action\n",
    "        q = policy_network(state[np.newaxis])[0][action_selected]\n",
    "        q_value.append(q)\n",
    "        \n",
    "        count+=1\n",
    "        \n",
    "    return q_value\n",
    "\n",
    "'''\n",
    "#Steps \n",
    "1. Same logic as with get_current_q, with the exception that if it's the terminal state the q value is 0.\n",
    "2. I'm using a greedy \n",
    "'''\n",
    "def get_q_prime(next_states,done):\n",
    "    \n",
    "    q_value = []\n",
    "    count=0\n",
    "    \n",
    "    for state in next_states:\n",
    "       \n",
    "        #need to check if it's the last state, for this I use the bool returned by the Envrionment \n",
    "        if (done[count] == True) :\n",
    "            q_value.append(tf.Variable(0.0 , dtype= tf.float32))\n",
    "        else:\n",
    "            q = target_network(state[np.newaxis]) \n",
    "            q_value.append(tf.reduce_max(q))#extract the float number from the tensorflow tensor\n",
    "        \n",
    "        count+=1\n",
    "    \n",
    "    return q_value\n",
    "\n",
    "def extract_tensor(experiences):\n",
    "    \n",
    "    experiences = Experience(*zip(*experiences))\n",
    "    \n",
    "    last_state = experiences.last_state\n",
    "    reward = experiences.reward\n",
    "    state = experiences.state\n",
    "    action = experiences.action\n",
    "    done = experiences.done\n",
    "    \n",
    "    \n",
    "    return last_state,action,state,reward,done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove existing file\n",
    "!rm -rf dqn_agent_weights\n",
    "\n",
    "#I will create a folder that will retain the weights of the policy network every 10 iterations\n",
    "import os\n",
    "if not os.path.exists('dqn_agent_weights'):\n",
    "    os.mkdir('dqn_agent_weights')\n",
    "\n",
    "file = 'dqn_agent_weights/weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c4bab8669df8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0;31m# Backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0;31m#compute the gradients with respect to the policy_network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolicy_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                     \u001b[0;31m#Then aplly the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1078\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1081\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_ones\u001b[0;34m(shape, dtype)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_ones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m   \u001b[0mas_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mas_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#have the agent and the environment all set up\n",
    "\n",
    "#plot class with a window average of 10\n",
    "plot = PlotRewards(10,agent)\n",
    "\n",
    "#Initialize the repllayBuffer\n",
    "replay_buffer = ReplayBuffer(max_buffer_size)\n",
    "\n",
    "#for each iteration -> 10 episodes\n",
    "for iteration in range(num_iterations):\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        #all the rewards received in one episode will be appended\n",
    "        rewards_per_episode = []\n",
    "        \n",
    "        #start state\n",
    "        state,reward,done = env.env_start()\n",
    "        \n",
    "        \n",
    "        #Iterate until the terminal state\n",
    "        while done != True:\n",
    "            \n",
    "            #Agent choose the action according to the policy network and the current state\n",
    "            action = agent.agent_step(state)\n",
    "            \n",
    "            #new state after executing the action chosen by the agent\n",
    "            next_state,reward,done = env.env_step(action)\n",
    "\n",
    "            #append the rewards \n",
    "            rewards_per_episode.append(reward)\n",
    "        \n",
    "            #build an experience\n",
    "            experience = Experience(state,action,next_state,reward,done)\n",
    "            \n",
    "            #add the experience to the replay buffer\n",
    "            replay_buffer.push(experience)\n",
    "            \n",
    "            state = next_state\n",
    "\n",
    "            \n",
    "            #When collected 32 experiences we can start optimizing our network\n",
    "            if(replay_buffer.can_provide_batch(batch_size)):\n",
    "                \n",
    "                #batch a sample of data (32 samples)\n",
    "                experiences_buffer = replay_buffer.batch_sample()\n",
    "                \n",
    "                #extract all the features of this experience batch\n",
    "                states,actions,next_states,rewards,dones = extract_tensor(experiences_buffer)\n",
    "            \n",
    "                #From now one we need a gradient tape to calculate the gradients of the policy with respect to the loss\n",
    "                #The loss is the MSE between the Q function and the Q' function\n",
    "                with tf.GradientTape() as tape:\n",
    "                    \n",
    "                    #watch the variables of the policy network\n",
    "                    tape.watch(policy_network.trainable_variables)\n",
    "                    \n",
    "                    #Get the q values for the taken states and actions\n",
    "                    current_q = get_current_q(states,actions)\n",
    "                    \n",
    "                    #Get the q prime values for the next states computed by the target network\n",
    "                    q_prime = get_q_prime(next_states,dones)\n",
    "                    \n",
    "                    #compute the target error\n",
    "                    target_error = tf.convert_to_tensor(rewards, dtype = tf.float32) + tf.Variable(agent.discount,dtype= tf.float32) * q_prime\n",
    "            \n",
    "                    #compute the loss function. I will use the Mean Squared error as the loss\n",
    "                    loss = MSE(target_error,current_q)\n",
    "            \n",
    "                    # Backprop\n",
    "                    #compute the gradients with respect to the policy_network\n",
    "                    grads = tape.gradient(loss,policy_network.trainable_variables)\n",
    "                    #Then aplly the gradients\n",
    "                    optimizer.apply_gradients(zip(grads, policy_network.trainable_variables))\n",
    "       \n",
    "        #1 in 1 episode -> plot the rewards as a function of time\n",
    "        episode_sum_reward = np.sum(np.asarray(rewards_per_episode))\n",
    "        tag = plot.plot(episode_sum_reward)\n",
    "        \n",
    "        if(tag == True):\n",
    "            print(\"Solved\")\n",
    "            env.env_end()\n",
    "            quit()\n",
    "        \n",
    "    #ended the X episodes, e.g, one iteration terminated\n",
    "    #In each iteration I update the target weights\n",
    "    target_network.set_weights(policy_network.get_weights())\n",
    "        \n",
    "    #if(iteration % 100 == 0):\n",
    "    #    tmp = file+str(iteration)+\"reward-\"+str(episode_sum_reward)+'.hdf5'\n",
    "    #    policy_network.save_weights(tmp)\n",
    "\n",
    "env.env_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![result](images/result.png \"Result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
