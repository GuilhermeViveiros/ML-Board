{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is intended to solve the CartPole-v0 problem with the vanilla apporach DQN.\n",
    "\n",
    "It will be a benchmark to future algorithms devoloped by Guilherme Viveiros.\n",
    "\n",
    "\n",
    "> Solved Requirements for CartPole: Considered solved when the average return is greater than or equal to 195.0 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym #envrionment to test the algorithms\n",
    "import numpy as np #vector calculations\n",
    "import tensorflow as tf #tensor / ML operations\n",
    "#!pip install jdc\n",
    "from tqdm import tqdm,trange\n",
    "import collections #collect experiences from real environment to sample within DQN\n",
    "import math#math\n",
    "import random\n",
    "\n",
    "from typing import Tuple,List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frist let's define our Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(tf.keras.models.Model): \n",
    "    def __init__(self,\n",
    "                 output_dim : int,\n",
    "                 hidden_layers : np.array,\n",
    "                 **kargs):\n",
    "        \"\"\" Initialize the agent\"\"\"\n",
    "        super(**kargs).__init__()\n",
    "        \n",
    "        #self.exploration_decay = exploration_decay\n",
    "        \n",
    "        self.hidden_layers = []\n",
    "        for i in hidden_layers:\n",
    "            self.hidden_layers.append(\n",
    "                tf.keras.layers.Dense(\n",
    "                    units = i,\n",
    "                    activation = 'elu',\n",
    "                    kernel_initializer = tf.keras.initializers.HeNormal(),\n",
    "                    #kernel_regularizer = tf.keras.regularizers.L2(),\n",
    "                    name = 'dense_'+str(i)))\n",
    "        \n",
    "        self.out =  tf.keras.layers.Dense(units = output_dim ,name='out')\n",
    "    \n",
    "    \n",
    "    def call(self,inputs):\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(inputs)\n",
    "            inputs = x\n",
    "        return self.out(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the next state, the associated reward and a boolean indicating if the agent reached the terminal state\n",
    "#I receive and output arrays because I'm using tf.numpy_function to wrap env_step to graph_mode and this functions\n",
    "#expects that the python function receives as its arguments an array and returns arrays as its outputs\n",
    "def env_step(action : np.array) -> Tuple[np.array,np.array,np.array]:\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    return ( state.astype(np.float32),\n",
    "             np.array(reward,dtype=np.int32),\n",
    "             np.array(done,dtype=np.int32)\n",
    "    )\n",
    "    \n",
    "def tf_env_step(action : tf.Tensor ) -> List[tf.Tensor]:\n",
    "    return tf.numpy_function(func = env_step, inp = [action], Tout = [tf.float32,tf.int32,tf.int32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's define the Replay Buffer to use within DQN\n",
    "\n",
    "> Also use name tuples to save experiments\n",
    "\n",
    "1. Each experiment contains: \n",
    "    1. State\n",
    "    2. Action\n",
    "    3. Next state\n",
    "    4. Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Experience = namedtuple('Experience',\n",
    "                   ('last_state', 'action', 'state', 'reward','done')\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For now, I can not run this algorithm in Graph Mode \n",
    "\n",
    "> No solution how to transform this ReplayBuffer Class into a tensorflow function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    #initialize the parameters and the memory buffer\n",
    "    def __init__(self,max_size):\n",
    "        self.memory = []\n",
    "        self.size = 0\n",
    "        self.max_size = max_size\n",
    "        \n",
    "    #push a experience to the memory buffer\n",
    "    def push(self,experience):\n",
    "        #case when the buffer is full\n",
    "        if(self.size >= self.max_size):\n",
    "            self.memory[self.size % self.max_size] = experience\n",
    "       #if it isn't full\n",
    "        else:\n",
    "            self.memory.append(experience)\n",
    "        \n",
    "        self.size +=1\n",
    "    \n",
    "    #check if the memory can provide a batch sample with a specific size\n",
    "    def can_provide_batch(self,batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        return self.size >= self.batch_size\n",
    "    \n",
    "    #return a batch sample\n",
    "    def batch_sample(self):\n",
    "        return random.sample(self.memory,self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Hyper-Parameters for the DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = env.action_space.n\n",
    "hidden_layers = [32,32]\n",
    "\n",
    "policy_network = DQN(action_space,hidden_layers)\n",
    "policy_network.build(tf.expand_dims(tf.constant(env.reset(),dtype=tf.float32),axis=0).shape)\n",
    "\n",
    "target_network = DQN(action_space,hidden_layers)\n",
    "#one needs to build the target network otherwise the set_weights function wont work\n",
    "target_network.build(tf.expand_dims(tf.constant(env.reset(),dtype=tf.float32),axis=0).shape)\n",
    "\n",
    "assert (len(target_network.trainable_variables) > 0) , \"Build the target network first by passing its shape, otherwise one can not access the weights in running time\"\n",
    "\n",
    "#set the target networks trainable parameter to false.\n",
    "#Every episode update this weights\n",
    "target_network.trainable = False\n",
    "target_network.set_weights(policy_network.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 32)(32,)(32, 32)(32,)(32, 2)(2,)"
     ]
    }
   ],
   "source": [
    "w = policy_network.get_weights()\n",
    "for i in range(len(w)):\n",
    "    print(w[i].shape, end = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_start = 1\n",
    "eps_end = 0.1\n",
    "eps_decay = 0.005\n",
    "\n",
    "max_buffer_size = 2000\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions\n",
    "\n",
    "1. Get the q value for the current state\n",
    "2. Get the q value for the next state, predicted by the target network\n",
    "3. Extract the tensors from the batch, respectivelly:\n",
    "    > State\\\n",
    "    > Next state\\\n",
    "    > Reward\\\n",
    "    > Action\\\n",
    "    > Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Steps \n",
    "1. Iterate over the \"current\" states from the batch. When I say current it's the actual states taken by the agent\n",
    "2. For each state, we check the action performed by the agent\n",
    "3. Aditionaly, we extract the Q value from the ploicy network for this state when taken this particular action\n",
    "'''\n",
    "def get_current_q(states : tf.Tensor,\n",
    "                  actions : tf.Tensor\n",
    "                 ) -> tf.Tensor : \n",
    "    \n",
    "    q_value = tf.TensorArray(dtype=tf.float32, size = len(states))\n",
    "    count = 0\n",
    "    \n",
    "    for state in states:\n",
    "        \n",
    "        #action selected in s\n",
    "        action_selected = actions[count]\n",
    "        \n",
    "        #extract the float number from the tensorflow tensor\n",
    "        #and the q value with respect to the chosen action\n",
    "        q = policy_network(state)[0,action_selected]\n",
    "        q_value.write(count,q).mark_used()\n",
    "        \n",
    "        count+=1\n",
    "        \n",
    "    return q_value.stack()\n",
    "\n",
    "'''\n",
    "#Steps \n",
    "1. Same logic as with get_current_q, with the exception that if it's the terminal state the q value is 0.\n",
    "2. I'm using a greedy \n",
    "'''\n",
    "def get_q_prime(next_states,done):\n",
    "    \n",
    "    q_prime = tf.TensorArray(dtype=tf.float32, size = len(next_states))\n",
    "    count=0\n",
    "    \n",
    "    for state in next_states:\n",
    "       \n",
    "        #need to check if it's the last state. To do this, I use the bool returned by the Envrionment \n",
    "        if (done[count] == True) :\n",
    "            q_prime.write(count,tf.Variable(0.0 , dtype= tf.float32)).mark_used()\n",
    "        else:\n",
    "            #only 1 sample in the batch, so get the first element\n",
    "            q = target_network(state)[0]\n",
    "            #extract the maximum Q from all posible choices\n",
    "            q_prime.write(count,tf.reduce_max(q,axis=-1)).mark_used()\n",
    "        \n",
    "        count+=1\n",
    "    \n",
    "    return q_prime.stack()\n",
    "\n",
    "def extract_tensor(experiences):\n",
    "    \n",
    "    experiences = Experience(*zip(*experiences))\n",
    "    \n",
    "    state = experiences.last_state\n",
    "    reward = experiences.reward\n",
    "    next_state = experiences.state\n",
    "    action = experiences.action\n",
    "    done = experiences.done\n",
    "    \n",
    "    \n",
    "    return state,action,next_state,reward,done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon greedy policy with decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(\n",
    "    rate : float,\n",
    "    state : tf.Tensor,\n",
    "    model : tf.keras.models.Model\n",
    ") -> tf.Tensor:\n",
    "    \n",
    "    \n",
    "    #exploitation case\n",
    "    if(tf.random.uniform(shape=[1]) > rate):\n",
    "        action_logits = model(state)\n",
    "        action = tf.argmax(action_logits[0], output_type = tf.int32)\n",
    "        \n",
    "    #exploration case\n",
    "    else:\n",
    "        action = tf.random.uniform(shape=[1],minval=0,maxval=action_space,dtype=tf.int32)[0]\n",
    "    \n",
    "    return action\n",
    "\n",
    "#state = tf.expand_dims(tf.constant(env.reset(),dtype=tf.float32),axis=0)\n",
    "#epsilon_greedy_policy(0.1,state,policy_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import Huber\n",
    "\n",
    "huber_loss = Huber(reduction='sum')\n",
    "\n",
    "gamma = tf.constant(0.95,dtype=tf.float32)\n",
    "\n",
    "#The loss is the Hubber loss between the Q function and the Q' function\n",
    "def compute_loss(states : tf.Tensor,\n",
    "                 next_states : tf.Tensor,\n",
    "                 actions : tf.Tensor,\n",
    "                 done : tf.Tensor,\n",
    "                 gamma : float\n",
    "                ) -> tf.Tensor:  \n",
    "    \n",
    "    #Get the q values for the taken states and actions\n",
    "    current_q = get_current_q(states,actions)       \n",
    "    #Get the q prime values for the next states computed by the target network\n",
    "    q_prime = get_q_prime(next_states,done)\n",
    "    #compute the target error\n",
    "    target_error = tf.cast(rewards, dtype = tf.float32) + gamma * q_prime\n",
    "    #compute the loss function. I will use the Hubber loss\n",
    "    loss = huber_loss(target_error,current_q)#/len(states)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(max_buffer_size)\n",
    "optimizer = tf.keras.optimizers.Adam(lr = 1e-2)\n",
    "\n",
    "#@tf.function\n",
    "def run_episode(\n",
    "    model : tf.keras.models.Model,\n",
    "    max_steps : int,   \n",
    "    rate : tf.float32,\n",
    "    initial_state : tf.Tensor,\n",
    "    #replay_buffer\n",
    ") -> tf.Tensor:\n",
    "                   \n",
    "    initial_state_shape = initial_state.shape\n",
    "    state = tf.expand_dims(initial_state,axis=0)\n",
    "    \n",
    "    rewards = tf.TensorArray(dtype=tf.int32, size = 0, dynamic_size = True)\n",
    "    \n",
    "    for i in tf.range(max_steps):\n",
    "        \n",
    "        \n",
    "        action = epsilon_greedy_policy(rate,state,model)\n",
    "        next_state,reward,done = tf_env_step(action)\n",
    "        \n",
    "        next_state.set_shape(initial_state_shape)\n",
    "        next_state = tf.expand_dims(next_state,axis=0)\n",
    "        \n",
    "        #append reward\n",
    "        rewards = rewards.write(i,reward)\n",
    "        \n",
    "         #build an experience\n",
    "        experience = Experience(state,action,next_state,reward,done)\n",
    "\n",
    "        #add the experience to the replay buffer\n",
    "        replay_buffer.push(experience)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        #if bool == True it means we reach the terminal state\n",
    "        if tf.cast(done,tf.bool):\n",
    "            break\n",
    "            \n",
    "    rewards = rewards.stack()\n",
    "        \n",
    "    return tf.math.reduce_sum(rewards)\n",
    "        \n",
    "#run_episode(model = policy_network, max_steps = 300, rate = 0.2, initial_state = tf.constant(env.reset(),dtype=tf.float32))#,replay_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One can run this, it can take a whil like 2000 episodes.\n",
    "\n",
    "> **Conclusions**:\\\n",
    "*DQN presents more instability than other algorithms like A2c*\n",
    "> Use other variants that were build to counter this issue such as:\\\n",
    "        1. Double DQN\\\n",
    "        2. Prioritized Experience Replay\\\n",
    "        3. Dueling DQN\\\n",
    "> Also, fine-tunne the hyper-paremeters, for example, use exponential scheduling decay in the learning rate to improve stability\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode = 1460:  15%|█▍        | 1460/10000 [05:31<32:20,  4.40it/s, episode_reward=199, moving_average=173, rate=0.10]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-355-e9defcdf4e4f>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(model, max_steps, rate, initial_state)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_greedy_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_env_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-353-23aebacf768b>\u001b[0m in \u001b[0;36mepsilon_greedy_policy\u001b[0;34m(rate, state, model)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0maction_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#exploration case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#number of episodes\n",
    "num_episodes = 10000\n",
    "#max steps per episode\n",
    "max_steps = 300\n",
    "# average reward\n",
    "weighted_average_reward = 0.0\n",
    "#keep track of the reards\n",
    "r = []\n",
    "\n",
    "#for each iteration -> 10 episodes\n",
    "with trange(num_episodes) as t:\n",
    "    \n",
    "    for episode in t:\n",
    "        \n",
    "        #calculate the epsilon rate\n",
    "        rate = tf.constant(eps_end + (eps_start - eps_end) * tf.math.exp(-1. * episode * eps_decay),tf.float32)\n",
    "        #start state\n",
    "        initial_state = tf.constant(env.reset(),dtype=tf.float32)\n",
    "        #change the type to int to compute the average reward in eager mode\n",
    "        episode_reward = int (run_episode(policy_network,max_steps,rate,initial_state))\n",
    "        r.append(episode_reward)\n",
    "    \n",
    "        t.set_description(f\"Episode = {episode + 1}\")\n",
    "        t.set_postfix(\n",
    "             episode_reward = episode_reward, moving_average = weighted_average_reward , rate = '%.2f'% float(rate)\n",
    "        )\n",
    "        \n",
    "\n",
    "        #When collected 32 experiences we can start optimizing our network        \n",
    "        if(replay_buffer.can_provide_batch(batch_size) and episode > 50): \n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                #watch the variables of the policy network\n",
    "                tape.watch(policy_network.trainable_variables)\n",
    "                #batch a sample of data\n",
    "                experiences_buffer = replay_buffer.batch_sample()\n",
    "                #extract all the features of this experience batch\n",
    "                states,actions,next_states,rewards,done = extract_tensor(experiences_buffer)\n",
    "                #compute the loss\n",
    "                loss = compute_loss(states,next_states,actions,done,gamma)\n",
    "\n",
    "            \n",
    "            # Backprop\n",
    "            #compute the gradients with respect to the policy_network\n",
    "            grads = tape.gradient(loss,policy_network.trainable_variables) \n",
    "            #to ensure None gradients one can add it zero vectors\n",
    "            #grads = [grad if grad is not None else tf.zeros_like(var) for var, grad in zip(policy_network.trainable_variables,grads)]\n",
    "       \n",
    "            #Then aplly the gradients\n",
    "            optimizer.apply_gradients(zip(grads, policy_network.trainable_variables))\n",
    "       \n",
    "        #10 in 10 episode -> calculate an weighted average reward\n",
    "        #if the agent reach as an 100 game average to 195 then the game is considered solved\n",
    "        weighted_average_reward = 0.01 * episode_reward + 0.99 * weighted_average_reward\n",
    "        \n",
    "        if(weighted_average_reward >= 195):\n",
    "                break\n",
    "        \n",
    "        #update the target network 10 in 10 episodes\n",
    "        if(episode % 10 == 0):\n",
    "            target_network.set_weights(policy_network.get_weights())\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![result](images/result.png \"Result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
